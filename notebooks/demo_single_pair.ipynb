{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo EfficientLoFTR on a single pair of images\n",
    "\n",
    "This notebook shows how to use the eloftr matcher with different model type and numerical precision on the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from src.utils.plotting import make_matching_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outdoor Example\n",
    "\n",
    "We recommend using our pre-trained model for input in outdoor environments because our model has only been trained on MegaDepth, and there exists a domain gap between indoor and outdoor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accelerated_features   glue-factory\t     SuperGluePretrainedNetwork\n",
      " EfficientLoFTR         LightGlue\t     SuperGlue-pytorch\n",
      " EfficientLoFTR_m       LoFTR\t\t     SuperPoint\n",
      " _Experiments\t        pytorch-superpoint\n",
      "'_Experiments copy'     RoMa\n"
     ]
    }
   ],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloftr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoFTR, full_default_cfg, opt_default_cfg, reparameter\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# You can choose model type in ['full', 'opt']\u001b[39;00m\n\u001b[32m      4\u001b[39m model_type = \u001b[33m'\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# 'full' for best quality, 'opt' for best efficiency\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.loftr import LoFTR, full_default_cfg, opt_default_cfg, reparameter\n",
    "\n",
    "# You can choose model type in ['full', 'opt']\n",
    "model_type = 'full' # 'full' for best quality, 'opt' for best efficiency\n",
    "\n",
    "# You can choose numerical precision in ['fp32', 'mp', 'fp16']. 'fp16' for best efficiency\n",
    "precision = 'fp32' # Enjoy near-lossless precision with Mixed Precision (MP) / FP16 computation if you have a modern GPU (recommended NVIDIA architecture >= SM_70).\n",
    "\n",
    "# You can also change the default values like thr. and npe (based on input image size)\n",
    "\n",
    "if model_type == 'full':\n",
    "    _default_cfg = deepcopy(full_default_cfg)\n",
    "elif model_type == 'opt':\n",
    "    _default_cfg = deepcopy(opt_default_cfg)\n",
    "    \n",
    "if precision == 'mp':\n",
    "    _default_cfg['mp'] = True\n",
    "elif precision == 'fp16':\n",
    "    _default_cfg['half'] = True\n",
    "    \n",
    "print(_default_cfg)\n",
    "matcher = LoFTR(config=_default_cfg)\n",
    "\n",
    "matcher.load_state_dict(torch.load(\"weights/eloftr_outdoor.ckpt\", weights_only=False)['state_dict'])\n",
    "matcher = reparameter(matcher) # no reparameterization will lead to low performance\n",
    "\n",
    "if precision == 'fp16':\n",
    "    matcher = matcher.half()\n",
    "\n",
    "matcher = matcher.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@48.547] global loadsave.cpp:268 findDecoder imread_('../_Experiments/clean_patches/01_0001_drone.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@48.547] global loadsave.cpp:268 findDecoder imread_('../_Experiments/clean_patches/01_0001_sat.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n\u001b[32m      5\u001b[39m img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m img0_raw = cv2.resize(img0_raw, (\u001b[43mimg0_raw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m1\u001b[39m]//\u001b[32m32\u001b[39m*\u001b[32m32\u001b[39m, img0_raw.shape[\u001b[32m0\u001b[39m]//\u001b[32m32\u001b[39m*\u001b[32m32\u001b[39m))  \u001b[38;5;66;03m# input size shuold be divisible by 32\u001b[39;00m\n\u001b[32m      7\u001b[39m img1_raw = cv2.resize(img1_raw, (img1_raw.shape[\u001b[32m1\u001b[39m]//\u001b[32m32\u001b[39m*\u001b[32m32\u001b[39m, img1_raw.shape[\u001b[32m0\u001b[39m]//\u001b[32m32\u001b[39m*\u001b[32m32\u001b[39m))\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m precision == \u001b[33m'\u001b[39m\u001b[33mfp16\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Load example images\n",
    "img0_pth = \"../_Experiments/clean_patches/01_0001_drone.png\"\n",
    "img1_pth = \"../_Experiments/clean_patches/01_0001_sat.png\"\n",
    "img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))  # input size shuold be divisible by 32\n",
    "img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\n",
    "\n",
    "if precision == 'fp16':\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].half().cuda() / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].half().cuda() / 255.\n",
    "else:\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255.\n",
    "batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "# Inference with EfficientLoFTR and get prediction\n",
    "with torch.no_grad():\n",
    "    if precision == 'mp':\n",
    "        with torch.autocast(enabled=True, device_type='cuda'):\n",
    "            matcher(batch)\n",
    "    else:\n",
    "        matcher(batch)\n",
    "    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "    mconf = batch['mconf'].cpu().numpy()\n",
    "\n",
    "# Draw\n",
    "if model_type == 'opt':\n",
    "    print(mconf.max())\n",
    "    mconf = (mconf - min(20.0, mconf.min())) / (max(30.0, mconf.max()) - min(20.0, mconf.min()))\n",
    "\n",
    "color = cm.jet(mconf)\n",
    "text = [\n",
    "    'LoFTR',\n",
    "    'Matches: {}'.format(len(mkpts0)),\n",
    "]\n",
    "fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualization to loftr_matches_visualization.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load example images\n",
    "img0_pth = \"EfficientLoFTR/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n",
    "img1_pth = \"EfficientLoFTR/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n",
    "img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))\n",
    "img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\n",
    "\n",
    "if precision == 'fp16':\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].half().cuda() / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].half().cuda() / 255.\n",
    "else:\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255.\n",
    "\n",
    "batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    if precision == 'mp':\n",
    "        with torch.autocast(enabled=True, device_type='cuda'):\n",
    "            matcher(batch)\n",
    "    else:\n",
    "        matcher(batch)\n",
    "    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "    mconf = batch['mconf'].cpu().numpy()\n",
    "\n",
    "# Normalize confidence for optical flow models\n",
    "if model_type == 'opt':\n",
    "    print(mconf.max())\n",
    "    mconf = (mconf - min(20.0, mconf.min())) / (max(30.0, mconf.max()) - min(20.0, mconf.min()))\n",
    "\n",
    "# Visualization\n",
    "color = cm.jet(mconf)\n",
    "text = [\n",
    "    'LoFTR',\n",
    "    f'Matches: {len(mkpts0)}',\n",
    "]\n",
    "fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)\n",
    "\n",
    "# Save figure\n",
    "output_path = \"loftr_matches_visualization.png\"\n",
    "fig.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Saved visualization to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from src.utils.plotting import make_matching_figure\n",
    "\n",
    "import glob\n",
    "import time\n",
    "from src.loftr import LoFTR, full_default_cfg, reparameter\n",
    "import gc  # for garbage collection\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_model(cfg):\n",
    "    _default_cfg = deepcopy(cfg)\n",
    "    model = LoFTR(config=_default_cfg)\n",
    "    model.load_state_dict(torch.load(\"weights/eloftr_outdoor.ckpt\", weights_only=False)['state_dict'], strict=False)\n",
    "    model = reparameter(model) # no reparameterization will lead to low performance\n",
    "    \n",
    "    return model.cuda().eval()\n",
    "\n",
    "def run(model, pairs):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for a,b in tqdm(pairs, total=len(pairs)):\n",
    "        Ia = torch.tensor(cv2.imread(a,0)/255.,dtype=torch.float32)[None,None].cuda()\n",
    "        Ib = torch.tensor(cv2.imread(b,0)/255.,dtype=torch.float32)[None,None].cuda()\n",
    "        with torch.no_grad(): model({'image0':Ia,'image1':Ib})\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter()-t0)/len(pairs)*1000\n",
    "\n",
    "def cleanup_model(model):\n",
    "    del model  # Delete the model reference\n",
    "    gc.collect()  # Run Python garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear unused memory in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline  ms/pair: 373.7322369997855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.5 ms/pair: 41.976432999945246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.5 ms/pair: 39.71856299904175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.6 ms/pair: 39.69303300254978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.7 ms/pair: 38.94955900250352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.8 ms/pair: 38.95496000041021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.9 ms/pair: 39.323901997704525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = list(zip(sorted(glob.glob('../../Data/Datasets/uav-visloc-loftr/val/images/*_drone.png')),\n",
    "                 sorted(glob.glob('../../Data/Datasets/uav-visloc-loftr/val/images/*_sat.png'))))\n",
    "\n",
    "pairs = pairs[:1] # for quick test\n",
    "\n",
    "model = get_model(full_default_cfg)\n",
    "print('baseline  ms/pair:', run(model, pairs))\n",
    "\n",
    "confidences = [0.5, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for c in confidences:\n",
    "    full_default_cfg['coarse']['depth_confidence'] = c\n",
    "    model = get_model(full_default_cfg)\n",
    "    model.depth_conf = c\n",
    "    print('conf', c, 'ms/pair:', run(model, pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline  ms/pair: 91.76463185001921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.5 ms/pair: 91.70715219999693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.6 ms/pair: 92.00609158997395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.7 ms/pair: 92.6327743100046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.8 ms/pair: 95.48924742000963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf 0.9 ms/pair: 100.12092717999622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AirSim\n",
    "import pandas as pd\n",
    "suffix = 'val'\n",
    "dataset_dir = '../../Data/AirSim/Ch1'\n",
    "pairs_df = pd.read_csv(os.path.join('../../Data/AirSim/Ch1/pair_csv/', 'pair_list_' + suffix + '.csv'))\n",
    "pairs_df = pairs_df[pairs_df['label'] == 1]\n",
    "\n",
    "pairs_df[\"path0\"] = pairs_df[\"image0\"].apply(lambda x: os.path.join(dataset_dir, x))\n",
    "pairs_df[\"path1\"] = pairs_df[\"image1\"].apply(lambda x: os.path.join(dataset_dir, x))\n",
    "pairs = pairs_df[[\"path0\", \"path1\"]].values.tolist()\n",
    "\n",
    "pairs = pairs[:100] # for quick test\n",
    "\n",
    "model = get_model(full_default_cfg)\n",
    "model.depth_conf = -1\n",
    "print('baseline  ms/pair:', run(model, pairs))\n",
    "\n",
    "confidences = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for c in confidences:\n",
    "    model.depth_conf = c\n",
    "    print('conf', c, 'ms/pair:', run(model, pairs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
